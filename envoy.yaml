static_resources:
  listeners:
  - name: listener_0
    address:
      socket_address:
        address: 0.0.0.0
        port_value: 10000
    filter_chains:
    - filters:
      - name: envoy.filters.network.http_connection_manager
        typed_config:
          "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
          stat_prefix: hello_service
          http_filters:
          - name: envoy.filters.http.llm_inference
            typed_config:
              "@type": type.googleapis.com/envoy.extensions.filters.http.llm_inference.v3.modelParameter
              n_thread: 16
              modelpath: {
                "qwen2": "/home/yuanjq/model/qwen2-7b-instruct-q5_k_m.gguf",
                "llama3": "/home/yuanjq/model/Meta-Llama-3-8B-Instruct-fp16.gguf",
              }
          - name: envoy.filters.http.router
          route_config:
            name: route
            virtual_hosts:
            - name: llm_inference_service
              domains: ["api.openai.com"]
              routes:
              - match:
                  prefix: "/v1/chat/completions"
                typed_per_filter_config:
                  envoy.filters.http.llm_inference:
                    "@type": type.googleapis.com/envoy.extensions.filters.http.llm_inference.v3.modelChosen
                    usemodel: 
                    - "qwen2"
                direct_response:
                  status: 200
                  body:
                    inline_string: "hello - prefix"
              - match:
                  prefix: "/v1/embeddings"
                direct_response:
                  status: 200
                  body:
                    inline_string: "hello - prefix"
      # original_dst_lb_config:
      #   use_http_header: true
                # direct_response:
                #   status: 200
                #   body:
                #     inline_string: "hello - prefix"






# static_resources:
#   listeners:
#   - name: listener_0
#     address:
#       socket_address:
#         address: 0.0.0.0
#         port_value: 10000
#     filter_chains:
#     - filters:
#       - name: envoy.filters.network.http_connection_manager
#         typed_config:
#           "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
#           stat_prefix: hello_service
#           http_filters:
#           # - name: sample
#           #   typed_config:
#           #     "@type": type.googleapis.com/sample.Decoder
#           #     key: via
#           #     val: /home/yuanjq/model/qwen1_5-0_5b-chat-q2_k.gguf
#           - name: envoy.filters.http.router
#           route_config:
#             name: route
#             virtual_hosts:
#             - name: llm_inference_service
#               domains: ["api.openai.com"]
#               typed_per_filter_config:
#                 envoy.filters.http.llm_inference:
#                   "@type": type.googleapis.com/envoy.extensions.filters.http.llm_inference.LLMInference
#                   key: via
#                   val: /home/yuanjq/model/qwen1_5-0_5b-chat-q2_k.gguf
#               routes:
#               - match:
#                   prefix: "/"
#                 direct_response:
#                   status: 200
#                   body:
#                     inline_string: "hello - prefix"
